{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Instalación de ODBC Driver 17 for SQL Server\n","\n","Necesario para la conexión a la base de datos"],"metadata":{"id":"FuLsmarNjU7g"}},{"cell_type":"code","source":["%%sh\n","curl https://packages.microsoft.com/keys/microsoft.asc | sudo tee /etc/apt/trusted.gpg.d/microsoft.asc\n","curl https://packages.microsoft.com/config/ubuntu/$(lsb_release -rs)/prod.list | sudo tee /etc/apt/sources.list.d/mssql-release.list\n","sudo apt-get update\n","sudo ACCEPT_EULA=Y apt-get install -y msodbcsql17"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sPQ497UVw3cp","executionInfo":{"status":"ok","timestamp":1751701734489,"user_tz":-120,"elapsed":19911,"user":{"displayName":"Christian TFG","userId":"01699797326416873661"}},"outputId":"0e4c271d-1f13-4962-c13a-71bc48635f7b"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["-----BEGIN PGP PUBLIC KEY BLOCK-----\n","Version: BSN Pgp v1.1.0.0\n","\n","mQENBFYxWIwBCADAKoZhZlJxGNGWzqV+1OG1xiQeoowKhssGAKvd+buXCGISZJwT\n","LXZqIcIiLP7pqdcZWtE9bSc7yBY2MalDp9Liu0KekywQ6VVX1T72NPf5Ev6x6DLV\n","7aVWsCzUAF+eb7DC9fPuFLEdxmOEYoPjzrQ7cCnSV4JQxAqhU4T6OjbvRazGl3ag\n","OeizPXmRljMtUUttHQZnRhtlzkmwIrUivbfFPD+fEoHJ1+uIdfOzZX8/oKHKLe2j\n","H632kvsNzJFlROVvGLYAk2WRcLu+RjjggixhwiB+Mu/A8Tf4V6b+YppS44q8EvVr\n","M+QvY7LNSOffSO6Slsy9oisGTdfE39nC7pVRABEBAAG0N01pY3Jvc29mdCAoUmVs\n","ZWFzZSBzaWduaW5nKSA8Z3Bnc2VjdXJpdHlAbWljcm9zb2Z0LmNvbT6JATQEEwEI\n","AB4FAlYxWIwCGwMGCwkIBwMCAxUIAwMWAgECHgECF4AACgkQ6z6Urb4SKc+P9gf/\n","diY2900wvWEgV7iMgrtGzx79W/PbwWiOkKoD9sdzhARXWiP8Q5teL/t5TUH6TZ3B\n","ENboDjwr705jLLPwuEDtPI9jz4kvdT86JwwG6N8gnWM8Ldi56SdJEtXrzwtlB/Fe\n","6tyfMT1E/PrJfgALUG9MWTIJkc0GhRJoyPpGZ6YWSLGXnk4c0HltYKDFR7q4wtI8\n","4cBu4mjZHZbxIO6r8Cci+xxuJkpOTIpr4pdpQKpECM6x5SaT2gVnscbN0PE19KK9\n","nPsBxyK4wW0AvAhed2qldBPTipgzPhqB2gu0jSryil95bKrSmlYJd1Y1XfNHno5D\n","xfn5JwgySBIdWWvtOI05gw==\n","=zPfd\n","-----END PGP PUBLIC KEY BLOCK-----\n","deb [arch=amd64,armhf,arm64] https://packages.microsoft.com/ubuntu/22.04/prod jammy mainGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n","Get:2 https://packages.microsoft.com/ubuntu/22.04/prod jammy InRelease [3,632 B]\n","Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n","Get:4 https://packages.microsoft.com/ubuntu/22.04/prod jammy/main arm64 Packages [66.9 kB]\n","Get:5 https://packages.microsoft.com/ubuntu/22.04/prod jammy/main amd64 Packages [228 kB]\n","Get:6 https://packages.microsoft.com/ubuntu/22.04/prod jammy/main armhf Packages [20.0 kB]\n","Get:7 https://packages.microsoft.com/ubuntu/22.04/prod jammy/main all Packages [1,252 B]\n","Hit:8 http://archive.ubuntu.com/ubuntu jammy InRelease\n","Get:9 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n","Get:10 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n","Hit:11 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n","Get:12 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n","Get:13 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease [24.3 kB]\n","Get:14 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,078 kB]\n","Hit:15 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n","Get:16 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [4,763 kB]\n","Get:17 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,804 kB]\n","Get:18 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n","Get:19 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy/main amd64 Packages [51.0 kB]\n","Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [56.4 kB]\n","Get:21 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,566 kB]\n","Get:22 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,404 kB]\n","Get:23 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,092 kB]\n","Get:24 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [48.5 kB]\n","Get:25 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,262 kB]\n","Get:26 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [4,917 kB]\n","Get:27 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,751 kB]\n","Fetched 33.5 MB in 6s (5,382 kB/s)\n","Reading package lists...\n","Reading package lists...\n","Building dependency tree...\n","Reading state information...\n","The following additional packages will be installed:\n","  odbcinst unixodbc\n","The following NEW packages will be installed:\n","  msodbcsql17 odbcinst unixodbc\n","0 upgraded, 3 newly installed, 0 to remove and 37 not upgraded.\n","Need to get 783 kB of archives.\n","After this operation, 164 kB of additional disk space will be used.\n","Get:1 https://packages.microsoft.com/ubuntu/22.04/prod jammy/main amd64 msodbcsql17 amd64 17.10.6.1-1 [746 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 odbcinst amd64 2.3.9-5ubuntu0.1 [9,930 B]\n","Get:3 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 unixodbc amd64 2.3.9-5ubuntu0.1 [26.7 kB]\n","Fetched 783 kB in 1s (1,150 kB/s)\n","Selecting previously unselected package odbcinst.\r\n","(Reading database ... \r(Reading database ... 5%\r(Reading database ... 10%\r(Reading database ... 15%\r(Reading database ... 20%\r(Reading database ... 25%\r(Reading database ... 30%\r(Reading database ... 35%\r(Reading database ... 40%\r(Reading database ... 45%\r(Reading database ... 50%\r(Reading database ... 55%\r(Reading database ... 60%\r(Reading database ... 65%\r(Reading database ... 70%\r(Reading database ... 75%\r(Reading database ... 80%\r(Reading database ... 85%\r(Reading database ... 90%\r(Reading database ... 95%\r(Reading database ... 100%\r(Reading database ... 126308 files and directories currently installed.)\r\n","Preparing to unpack .../odbcinst_2.3.9-5ubuntu0.1_amd64.deb ...\r\n","Unpacking odbcinst (2.3.9-5ubuntu0.1) ...\r\n","Selecting previously unselected package unixodbc.\r\n","Preparing to unpack .../unixodbc_2.3.9-5ubuntu0.1_amd64.deb ...\r\n","Unpacking unixodbc (2.3.9-5ubuntu0.1) ...\r\n","Selecting previously unselected package msodbcsql17.\r\n","Preparing to unpack .../msodbcsql17_17.10.6.1-1_amd64.deb ...\r\n","debconf: unable to initialize frontend: Dialog\r\n","debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\r\n","debconf: falling back to frontend: Readline\r\n","Unpacking msodbcsql17 (17.10.6.1-1) ...\r\n","Setting up odbcinst (2.3.9-5ubuntu0.1) ...\r\n","Setting up unixodbc (2.3.9-5ubuntu0.1) ...\r\n","Setting up msodbcsql17 (17.10.6.1-1) ...\r\n","odbcinst: Driver installed. Usage count increased to 1. \r\n","    Target directory is /etc\r\n","Processing triggers for man-db (2.10.2-1) ...\r\n"]},{"output_type":"stream","name":"stderr","text":["  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100   975  100   975    0     0   8111      0 --:--:-- --:--:-- --:--:--  8057\n","  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100    88  100    88    0     0   1306      0 --:--:-- --:--:-- --:--:--  1313\n","W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n","debconf: unable to initialize frontend: Dialog\n","debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 3.)\n","debconf: falling back to frontend: Readline\n","debconf: unable to initialize frontend: Readline\n","debconf: (This frontend requires a controlling tty.)\n","debconf: falling back to frontend: Teletype\n","dpkg-preconfigure: unable to re-open stdin: \n"]}]},{"cell_type":"markdown","source":["# Dependencias necesarias para las predicciones e insertar los resultados en la base de datos"],"metadata":{"id":"85aXhr4DjuxF"}},{"cell_type":"code","source":["!pip install pysentimiento --no-deps\n","!pip install gender_guesser\n","!pip install pyodbc\n","!pip install langdetect\n","!pip install emoji\n","!pip install deep_translator"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bqkHunrHw5MP","executionInfo":{"status":"ok","timestamp":1751619370816,"user_tz":-120,"elapsed":44484,"user":{"displayName":"Jenifer Vasquez","userId":"07982682319972899452"}},"outputId":"494b5d38-9b27-4a3c-dd8c-135bffe8698a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pysentimiento\n","  Downloading pysentimiento-0.7.3-py3-none-any.whl.metadata (7.7 kB)\n","Downloading pysentimiento-0.7.3-py3-none-any.whl (39 kB)\n","Installing collected packages: pysentimiento\n","Successfully installed pysentimiento-0.7.3\n","Collecting gender_guesser\n","  Downloading gender_guesser-0.4.0-py2.py3-none-any.whl.metadata (3.0 kB)\n","Downloading gender_guesser-0.4.0-py2.py3-none-any.whl (379 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m379.3/379.3 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: gender_guesser\n","Successfully installed gender_guesser-0.4.0\n","Collecting pyodbc\n","  Downloading pyodbc-5.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.7 kB)\n","Downloading pyodbc-5.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (346 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.2/346.2 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pyodbc\n","Successfully installed pyodbc-5.2.0\n","Collecting langdetect\n","  Downloading langdetect-1.0.9.tar.gz (981 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\n","Building wheels for collected packages: langdetect\n","  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993223 sha256=f25b848b18039195b08fbcb24fdde4cee9e2cb8a8c5ade97a7029905d578cd98\n","  Stored in directory: /root/.cache/pip/wheels/0a/f2/b2/e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\n","Successfully built langdetect\n","Installing collected packages: langdetect\n","Successfully installed langdetect-1.0.9\n","Collecting emoji\n","  Downloading emoji-2.14.1-py3-none-any.whl.metadata (5.7 kB)\n","Downloading emoji-2.14.1-py3-none-any.whl (590 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m590.6/590.6 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: emoji\n","Successfully installed emoji-2.14.1\n","Collecting deep_translator\n","  Downloading deep_translator-1.11.4-py3-none-any.whl.metadata (30 kB)\n","Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /usr/local/lib/python3.11/dist-packages (from deep_translator) (4.13.4)\n","Requirement already satisfied: requests<3.0.0,>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from deep_translator) (2.32.3)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep_translator) (2.7)\n","Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep_translator) (4.14.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (2025.6.15)\n","Downloading deep_translator-1.11.4-py3-none-any.whl (42 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: deep_translator\n","Successfully installed deep_translator-1.11.4\n"]}]},{"cell_type":"markdown","source":["# Entrenamiento del modelo\n","\n","IMPORTANTE: subir al entorno de trabajo el dataset"],"metadata":{"id":"kl8WRV8vkCCR"}},{"cell_type":"code","source":["import pandas as pd\n","import tensorflow as tf\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n","from sklearn.metrics import classification_report, confusion_matrix\n","import numpy as np\n","import os, pickle, tqdm\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import re\n","import nltk\n","from nltk.corpus import wordnet, stopwords\n","import random\n","from transformers import BertTokenizer, TFBertModel\n","\n","# Sinónimos\n","nltk.download('wordnet', quiet=True)\n","nltk.download('omw-1.4', quiet=True)\n","# Artículos, conjunciones, etc.\n","nltk.download('stopwords', quiet=True)\n","\n","STOPWORDS_ES = set(stopwords.words('spanish'))\n","\n","# Preprocesamiento de texto\n","def clean_text(text):\n","    text = text.lower()\n","    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n","    text = re.sub(r'\\@w+|\\#', '', text)\n","    text = re.sub(r'[^\\w\\s]', '', text)\n","    text = text.strip()\n","    return text\n","\n","# Aumento de datos\n","def augment_text_synonym_replacement_es(text, p=0.1):\n","    words = text.split()\n","    new_words = words[:]\n","\n","    # Generar la lista de palabras candidatas (sin stopwords y sin duplicados)\n","    unique_words = set()\n","    for word in words:\n","        if word not in STOPWORDS_ES:\n","            unique_words.add(word)\n","\n","    candidate_words = list(unique_words)\n","    random.shuffle(candidate_words)\n","\n","    # Determinar cuántas palabras se reemplazarán (1 a n)\n","    num_replaced_words = max(1, int(len(candidate_words) * p))\n","\n","    # Reemplazar palabras por sinónimos\n","    for random_word in candidate_words[:num_replaced_words]:\n","        synonyms = []\n","        for syn in wordnet.synsets(random_word, lang='spa'):\n","            for lemma in syn.lemmas('spa'):\n","                synonym = lemma.name().replace(\"_\", \" \")\n","                if synonym != random_word and synonym not in synonyms:\n","                    synonyms.append(synonym)\n","        if synonyms:\n","            chosen_synonym = random.choice(synonyms)\n","            for i, w in enumerate(new_words):\n","                if w == random_word:\n","                    new_words[i] = chosen_synonym\n","    return \" \".join(new_words)\n","\n","# Cargar datos\n","df = pd.read_csv(\"2kreviewsfiltered.csv\")\n","df = df[[\"text\", \"categoryName\"]].dropna()\n","df[\"text\"] = df[\"text\"].astype(str).apply(clean_text)\n","df = df[df[\"text\"] != \"\"]\n","\n","# Codificar etiquetas\n","label_encoder = LabelEncoder()\n","df[\"encoded_label\"] = label_encoder.fit_transform(df[\"categoryName\"])\n","y_encoded = df[\"encoded_label\"].values\n","\n","# Tokenizer y modelo BERT Español\n","bert_model = TFBertModel.from_pretrained(\"dccuchile/bert-base-spanish-wwm-cased\")\n","tokenizer = BertTokenizer.from_pretrained(\"dccuchile/bert-base-spanish-wwm-cased\")\n","\n","def get_bert_embeddings(texts, max_len=128, batch_size=16):\n","    embeddings = []\n","    for i in tqdm.tqdm(range(0, len(texts), batch_size)):\n","        batch = texts[i:i + batch_size]\n","        tokens = tokenizer(batch, padding='max_length', truncation=True,\n","                           max_length=max_len, return_tensors='tf')\n","        outputs = bert_model(**tokens)\n","        cls_embeddings = tf.reduce_mean(outputs.last_hidden_state, axis=1)\n","        embeddings.append(cls_embeddings.numpy())\n","    return np.concatenate(embeddings)\n","\n","# División del dataset en subconjuntos\n","X_texts = df[\"text\"].tolist()\n","X_train_text, X_test_text, y_train, y_test = train_test_split(\n","    X_texts, y_encoded, test_size=0.15, random_state=42, stratify=y_encoded\n",")\n","X_train_text, X_val_text, y_train, y_val = train_test_split(\n","    X_train_text, y_train, test_size=0.1765, random_state=42, stratify=y_train\n",")\n","\n","# Aumento de datos\n","augmented_X_train = list(X_train_text)\n","augmented_y_train = list(y_train)\n","\n","for i in tqdm.tqdm(range(len(X_train_text))):\n","    augmented_text = augment_text_synonym_replacement_es(X_train_text[i], p=0.2)\n","    if augmented_text != X_train_text[i]:\n","        augmented_X_train.append(augmented_text)\n","        augmented_y_train.append(y_train[i])\n","\n","# TF-IDF vectorizer\n","tfidf_vectorizer = TfidfVectorizer(max_features=2000, ngram_range=(1,2), min_df=5, max_df=0.8)\n","\n","# Embeddings BERT y TF-IDF\n","X_train_bert = get_bert_embeddings(augmented_X_train)\n","X_train_tfidf = tfidf_vectorizer.fit_transform(augmented_X_train).toarray()\n","X_train_combined = np.concatenate([X_train_bert, X_train_tfidf], axis=1)\n","\n","X_val_bert = get_bert_embeddings(X_val_text)\n","X_val_tfidf = tfidf_vectorizer.transform(X_val_text).toarray()\n","X_val_combined = np.concatenate([X_val_bert, X_val_tfidf], axis=1)\n","\n","X_test_bert = get_bert_embeddings(X_test_text)\n","X_test_tfidf = tfidf_vectorizer.transform(X_test_text).toarray()\n","X_test_combined = np.concatenate([X_test_bert, X_test_tfidf], axis=1)\n","\n","# One-hot encoding\n","y_train_cat = to_categorical(augmented_y_train)\n","y_val_cat = to_categorical(y_val)\n","y_test_cat = to_categorical(y_test)\n","\n","# Modelo\n","model = Sequential([\n","    Dense(512, activation='relu', input_shape=(X_train_combined.shape[1],)),\n","    Dropout(0.3),\n","    Dense(64, activation='relu'),\n","    Dropout(0.2),\n","    Dense(y_train_cat.shape[1], activation='softmax')\n","])\n","\n","optimizer = tf.keras.optimizers.Adam(learning_rate=0.00017954)\n","\n","model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n","\n","early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, verbose=1)\n","reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=1, min_lr=1e-6, verbose=1)\n","\n","# Entrenamiento\n","history = model.fit(X_train_combined, y_train_cat,\n","                    validation_data=(X_val_combined, y_val_cat),\n","                    epochs=25,\n","                    batch_size=32,\n","                    callbacks=[early_stop, reduce_lr],\n","                    verbose=1)\n","\n","# Evaluación\n","y_pred_probs = model.predict(X_test_combined)\n","y_pred = np.argmax(y_pred_probs, axis=1)\n","print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n","\n","cm = confusion_matrix(y_test, y_pred)\n","cm_normalized = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n","plt.figure(figsize=(12, 10))\n","sns.heatmap(cm_normalized, annot=True, fmt=\".2f\", cmap=\"Blues\",\n","            xticklabels=label_encoder.classes_,\n","            yticklabels=label_encoder.classes_)\n","plt.title(\"Matriz de Confusión Normalizada\")\n","plt.xlabel(\"Predicción\")\n","plt.ylabel(\"Clase Real\")\n","plt.xticks(rotation=90)\n","plt.tight_layout()\n","plt.show()\n","\n","# Guardar\n","print(\"\\n--- Guardando modelo y componentes ---\")\n","model.save(\"review_model_combined_espanol.keras\")\n","with open(\"tfidf_vectorizer.pkl\", \"wb\") as f:\n","    pickle.dump(tfidf_vectorizer, f)\n","with open(\"label_encoder.pkl\", \"wb\") as f:\n","    pickle.dump(label_encoder, f)\n","with open(\"bert_tokenizer.pkl\", \"wb\") as f:\n","    pickle.dump(tokenizer, f)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7YelQ3cxwlNz","outputId":"6cef6bad-13ca-4dd7-afe2-d1047f9eb51f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 29937/29937 [00:04<00:00, 6068.04it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Obteniendo embeddings BERT y TF-IDF para train...\n"]},{"output_type":"stream","name":"stderr","text":[" 25%|██▌       | 215/854 [09:58<30:29,  2.86s/it]"]}]},{"cell_type":"markdown","source":["# Reglas de firewall\n","\n","Para que se pueda realizar la conexión a la base de datos es necesario incluir la IP de Colab en las reglas de firewall de Azure.\n","\n","Esto se puede hacer accediendo al portal de Azure y una vez alli, SQL Server --> Redes --> Reglas de firewall (haciendo scroll).\n","\n","El nombre de regla es indiferente, solo hay que copiar la IP de abajo en dirección IPv4 de inicio y de final."],"metadata":{"id":"txFn-NTakMEp"}},{"cell_type":"code","source":["!curl ifconfig.me"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kXXT2huYlB7O","executionInfo":{"status":"ok","timestamp":1751702114484,"user_tz":-120,"elapsed":298,"user":{"displayName":"Christian TFG","userId":"01699797326416873661"}},"outputId":"1b5dd563-9790-45e3-a1d4-a8d050220c10"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["35.186.146.45"]}]},{"cell_type":"markdown","source":["# Predicciones de los tipos de recursos y creación del script para insertar los datos en la base de datos\n","\n","IMPORTANTE: La celda anterior genera los siguientes ficheros, en caso de no haberla ejecutado, se han de cargar externamente.\n","\n","- Fichero .keras del modelo\n","- Fichero .pkl del vectorizer\n","- Fichero .pkl del label encoder\n","\n","También hay que subir el fichero preprocesado con las reseñas limpias en formato CSV el cual no se obtiene de la celda anterior.\n"],"metadata":{"id":"rp0d_wa6lv1s"}},{"cell_type":"code","source":["import pandas as pd\n","import json\n","import re\n","import pickle\n","import numpy as np\n","import tensorflow as tf\n","from transformers import TFBertModel, BertTokenizer\n","import pyodbc\n","from tensorflow.keras.models import load_model\n","import gender_guesser.detector as gender\n","from pysentimiento import create_analyzer\n","\n","def clean_text(text):\n","    text = text.lower()\n","    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n","    text = re.sub(r'\\@\\w+|\\#', '', text)\n","    text = re.sub(r'[^\\w\\s]', '', text)\n","    return text.strip()\n","\n","def get_bert_embedding(text, max_len=128):\n","    tokens = tokenizer([text], padding='max_length', truncation=True, max_length=max_len, return_tensors='tf')\n","    outputs = bert_model(**tokens)\n","    bert_vec = tf.reduce_mean(outputs.last_hidden_state, axis=1).numpy()\n","    return bert_vec\n","\n","def predecir_categoria(nombre, texto):\n","    texto_completo = f\"{(nombre + ' ') * 50}{texto}\"\n","    texto_limpio = clean_text(texto_completo)\n","    bert_vec = get_bert_embedding(texto_limpio)\n","    tfidf_vec = tfidf_vectorizer.transform([texto_limpio]).toarray()\n","    combined_vec = np.concatenate([bert_vec, tfidf_vec], axis=1)\n","    pred = model.predict(combined_vec)\n","    index_top1 = pred.argmax(axis=1)[0]\n","    return label_encoder.inverse_transform([index_top1])[0]\n","\n","def safe_get(value):\n","    if pd.isna(value):\n","        return ''\n","    return str(value).replace('\\n', ' ').replace('\\r', ' ').strip()\n","\n","def sql_value(val):\n","    if val == \"NULL\" or val == '' or pd.isna(val):\n","        return \"NULL\"\n","    val = str(val)\n","    val = val.replace(\"'\", \"''\")\n","    val = val.replace(\";\", \" \")\n","    val = val.replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n","\n","    return f\"'{val.strip()}'\"\n","\n","\n","def parse_coordinates(coord_str):\n","    try:\n","        coord_json = json.loads(coord_str.replace(\"'\", '\"'))\n","        return {\n","            'lat': float(coord_json.get('lat', 0)),\n","            'lng': float(coord_json.get('lng', 0))\n","        }\n","    except:\n","        return {}\n","\n","def to_iso_ms_z(dt_str):\n","    from datetime import datetime\n","    try:\n","        dt = datetime.fromisoformat(dt_str.replace('Z', ''))\n","        return dt.strftime('%Y-%m-%dT%H:%M:%S.%f')[:-3] + 'Z'\n","    except:\n","        return dt_str\n","\n","detector = gender.Detector(case_sensitive=False)\n","def predecir_genero(nombre, flag=False):\n","    if nombre == \"NULL\" or pd.isna(nombre) or nombre == '':\n","        return \"NULL\"\n","    try:\n","        if flag:\n","            nombre = nombre.strip().split()[0]\n","        genero = detector.get_gender(nombre)\n","        if genero in ['male', 'mostly_male']:\n","            return \"'Masculino'\"\n","        elif genero in ['female', 'mostly_female']:\n","            return \"'Femenino'\"\n","        else:\n","            if not flag:\n","                return predecir_genero(nombre, True)\n","    except Exception:\n","        pass\n","    return \"'Desconocido'\"\n","\n","analyzer = create_analyzer(task=\"sentiment\", lang=\"es\")\n","def get_polaridad(texto):\n","    resultado = analyzer.predict(texto)\n","    return (resultado.probas[\"POS\"] * 4 + resultado.probas[\"NEU\"] * 0 + resultado.probas[\"NEG\"] * -4)\n","\n","# --------- Carga de modelos ---------\n","MODEL_PATH = \"review_model_combined_espanol.keras\"\n","TFIDF_PATH = \"tfidf_vectorizer.pkl\"\n","LABEL_ENCODER_PATH = \"label_encoder.pkl\"\n","\n","print(\"Cargando modelo y recursos...\")\n","model = load_model(MODEL_PATH)\n","with open(TFIDF_PATH, \"rb\") as f:\n","    tfidf_vectorizer = pickle.load(f)\n","with open(LABEL_ENCODER_PATH, \"rb\") as f:\n","    label_encoder = pickle.load(f)\n","bert_model = TFBertModel.from_pretrained(\"dccuchile/bert-base-spanish-wwm-cased\")\n","tokenizer = BertTokenizer.from_pretrained(\"dccuchile/bert-base-spanish-wwm-cased\")\n","print(\"Modelo y recursos cargados.\")\n","\n","# --------- Conexión a la DB ---------\n","conn = pyodbc.connect(\n","    \"DRIVER={ODBC Driver 17 for SQL Server};SERVER=ubu-reviews-dti.database.windows.net;PORT=1433;\"\n","    \"DATABASE=reviews;UID=ubuadmin;PWD=UBUreviews2025;Encrypt=yes;TrustServerCertificate=no;Authentication=SqlPassword;TLSVersion=1.3;\"\n",")\n","cursor = conn.cursor()\n","\n","cursor.execute(\"SELECT POI_ID, nombre, categoria FROM POI\")\n","poi_existentes = {row.nombre: (row.POI_ID, row.categoria) for row in cursor.fetchall()}\n","\n","df = pd.read_csv('cleanReviews.csv', dtype=str)\n","\n","city = None\n","state = None\n","\n","if 'city' in df.columns and df['city'].notna().any():\n","    city = df.loc[df['city'].notna(), 'city'].iloc[0]\n","else:\n","    city = \"Unknown\"\n","\n","if 'state' in df.columns and df['state'].notna().any():\n","    state = df.loc[df['state'].notna(), 'state'].iloc[0]\n","else:\n","    state = city\n","\n","poi_inserts = []\n","reviews_inserts = []\n","cursor.execute(\"SELECT MAX(POI_ID) FROM POI\")\n","max_poi_id_row = cursor.fetchone()\n","max_poi_id = max_poi_id_row[0] if max_poi_id_row and max_poi_id_row[0] else 0\n","next_poi_id = max_poi_id + 1\n","\n","poi_embeddings = {}\n","poi_coords = {}\n","\n","grouped = df.groupby('placeId')\n","\n","# --------- PROCESAR POIS ----------\n","\n","for place_id, group in grouped:\n","    nombre = safe_get(group['title'].iloc[0]) or place_id\n","    categoria = safe_get(group['categoryName'].iloc[0]) or \"NULL\"\n","    municipio = city\n","\n","    if nombre in poi_existentes:\n","        POI_ID, categoria_existente = poi_existentes[nombre]\n","        categoria = categoria_existente\n","    else:\n","        # Concatenamos textos de reviews\n","        textos_reviews_poi = group['text'].fillna('').tolist()\n","        texto_concatenado = \" \".join(textos_reviews_poi).strip()\n","        if texto_concatenado == '':\n","            texto_concatenado = nombre\n","\n","        embedding = get_bert_embedding(texto_concatenado)[0]\n","        poi_embeddings[nombre] = embedding\n","\n","        POI_ID = next_poi_id\n","        next_poi_id += 1\n","        categoria = predecir_categoria(nombre, texto_concatenado)\n","\n","        # Parsear coordenadas del campo 'location'\n","        coords = parse_coordinates(safe_get(group['location'].iloc[0]))\n","        poi_coords[nombre] = (coords.get('lat', 0.0), coords.get('lng', 0.0))\n","        latitud = coords.get('lat', \"NULL\")\n","        longitud = coords.get('lng', \"NULL\")\n","\n","        totalScore = safe_get(group['totalScore'].iloc[0])\n","        totalScore_val = \"NULL\" if totalScore in ('', \"NULL\") else totalScore\n","\n","        val_str = f\"({POI_ID}, {sql_value(nombre)}, {sql_value(categoria)}, {sql_value(municipio)}, {latitud}, {longitud}, {totalScore_val})\"\n","        poi_inserts.append(val_str)\n","        poi_existentes[nombre] = (POI_ID, categoria)\n","\n","# Mapeo placeId a nombre para facilitar inserción reviews\n","placeid_to_nombre = {place_id: safe_get(group['title'].iloc[0]) or place_id for place_id, group in grouped}\n","\n","# --------- PROCESAR REVIEWS ----------\n","\n","batch_size = 950\n","for start in range(0, len(df), batch_size):\n","    batch = df.iloc[start:start+batch_size]\n","    vals = []\n","    for _, row in batch.iterrows():\n","        place_id = safe_get(row.get('placeId', ''))\n","        nombre = placeid_to_nombre.get(place_id)\n","        if not nombre or nombre not in poi_existentes:\n","            continue\n","        POI_ID, categoria = poi_existentes[nombre]\n","\n","        text = safe_get(row.get('text', ''))\n","        if text == '':\n","            continue\n","\n","        text_original = safe_get(row.get('text_original', ''))\n","        if text_original == '':\n","            continue\n","\n","        idioma_final = safe_get(row.get('originalLanguage', ''))\n","\n","        valoracion = safe_get(row.get('stars', ''))\n","        valoracion_val = \"NULL\" if valoracion in (\"\", \"NULL\") else valoracion\n","\n","        fecha_raw = safe_get(row.get('publishedAtDate', ''))\n","        fecha = sql_value(to_iso_ms_z(fecha_raw))\n","\n","        user_name = safe_get(row.get('name', ''))\n","        user = sql_value(user_name)\n","        genero = predecir_genero(user_name)\n","\n","        try:\n","            polaridad = get_polaridad(text)\n","        except:\n","            polaridad = \"NULL\"\n","\n","        stars_val = valoracion_val\n","\n","        vals.append(f\"({POI_ID}, {sql_value(text_original)}, {valoracion_val}, {fecha}, {sql_value(idioma_final)}, {user}, {genero}, {polaridad}, {stars_val})\")\n","\n","    if vals:\n","        reviews_inserts.append(\n","            \"INSERT INTO reviews (POI_ID, texto, valoracion, fecha, idioma, [user], genero, polarity, stars) VALUES\\n\" +\n","            \",\\n\".join(vals) + \";\"\n","        )\n","\n","output_sql = \"datos.sql\"\n","with open(output_sql, \"w\", encoding=\"utf-8\") as f:\n","    if poi_inserts:\n","        f.write(\"INSERT INTO POI (POI_ID, nombre, categoria, municipio, latitud, longitud, totalScore) VALUES\\n\")\n","        f.write(\",\\n\".join(poi_inserts) + \";\\n\\n\")\n","    if reviews_inserts:\n","        f.write(\"\\n\\n\".join(reviews_inserts))\n","\n","print(f\"Archivo '{output_sql}' generado correctamente.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3m21fozqw0HR","executionInfo":{"status":"ok","timestamp":1751626639215,"user_tz":-120,"elapsed":80977,"user":{"displayName":"Jenifer Vasquez","userId":"07982682319972899452"}},"outputId":"e78f5cb3-4ed4-47ee-fc7a-46b5f84448a7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cargando modelo y recursos...\n","Modelo y recursos cargados.\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 205ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n","Archivo 'datos.sql' generado correctamente.\n"]}]},{"cell_type":"markdown","source":["# Función para cargar ficheros .sql en la base de datos"],"metadata":{"id":"JTDhLigDlyWf"}},{"cell_type":"code","source":["def cargar_sql_en_bd(ruta_sql, cursor, conn):\n","    with open(ruta_sql, \"r\", encoding=\"utf-8\") as file:\n","        script = file.read()\n","\n","    comandos = [cmd.strip() for cmd in script.split(\";\") if cmd.strip()]\n","\n","    # Detectar si hay insert en tabla POI para manejar IDENTITY_INSERT\n","    activar_identity = any(\"INSERT INTO POI\" in cmd.upper() for cmd in comandos)\n","\n","    if activar_identity:\n","        try:\n","            cursor.execute(\"SET IDENTITY_INSERT POI ON;\")\n","            conn.commit()\n","        except Exception as e:\n","            print(f\"Error activando IDENTITY_INSERT ON: {e}\")\n","\n","    for comando in comandos:\n","        try:\n","            cursor.execute(comando)\n","        except Exception as e:\n","            print(f\"Error ejecutando comando: {comando[:100]}...\\n{e}\")\n","\n","    if activar_identity:\n","        try:\n","            cursor.execute(\"SET IDENTITY_INSERT POI OFF;\")\n","            conn.commit()\n","        except Exception as e:\n","            print(f\"Error desactivando IDENTITY_INSERT OFF: {e}\")\n","\n","    conn.commit()\n","    print(f\"Archivo '{ruta_sql}' cargado correctamente en la base de datos.\")"],"metadata":{"id":"HG5vQBj6qsrb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Carga del fichero con las reseñas y los recursos clasificados"],"metadata":{"id":"kIV62PQ6l1jz"}},{"cell_type":"code","source":["cargar_sql_en_bd(\"datos.sql\", cursor, conn)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LMplQE5m84Z1","executionInfo":{"status":"ok","timestamp":1751626696471,"user_tz":-120,"elapsed":3339,"user":{"displayName":"Jenifer Vasquez","userId":"07982682319972899452"}},"outputId":"88ca4be7-ba44-4854-960e-6151eb4144f7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Archivo 'datos.sql' cargado correctamente en la base de datos.\n"]}]},{"cell_type":"markdown","source":["# Sistema de recomendación mediante clusters\n","\n","Agrupa los recursos en x grupos por su similitud en las reseñas y localización.\n","Esta cantidad x viene dada por el literal almacenado en la variable ```n_clusters```"],"metadata":{"id":"eEprqbffl6p3"}},{"cell_type":"code","source":["from sklearn.cluster import KMeans\n","from sklearn.preprocessing import StandardScaler\n","\n","n_clusters = 20\n","\n","# Conexión a la BD\n","conn = pyodbc.connect(\n","    \"DRIVER={ODBC Driver 17 for SQL Server};SERVER=ubu-reviews-dti.database.windows.net;PORT=1433;\"\n","    \"DATABASE=reviews;UID=ubuadmin;PWD=UBUreviews2025;Encrypt=yes;TrustServerCertificate=no;Authentication=SqlPassword;TLSVersion=1.3;\"\n",")\n","cursor = conn.cursor()\n","\n","# Obtener POI_ID y textos concatenados de reviews agrupados por POI_ID\n","query = \"\"\"\n","SELECT POI_ID, STRING_AGG(texto, ' ') AS full_text\n","FROM reviews\n","GROUP BY POI_ID\n","\"\"\"\n","cursor.execute(query)\n","rows = cursor.fetchall()\n","\n","# Obtener coords de POI para los POI_IDs que tenemos\n","poi_ids = [row.POI_ID for row in rows]\n","poi_coords = {}\n","poi_nombres = {}\n","if poi_ids:\n","    format_strings = ','.join(['?'] * len(poi_ids))\n","    cursor.execute(f\"SELECT POI_ID, nombre, latitud, longitud FROM POI WHERE POI_ID IN ({format_strings})\", poi_ids)\n","    for poi_id, nombre, lat, lng in cursor.fetchall():\n","        poi_coords[poi_id] = (lat, lng)\n","        poi_nombres[poi_id] = nombre\n","\n","# Generar embeddings para cada POI concatenando sus textos\n","poi_embeddings = {}\n","poi_coord_list = []\n","poi_id_list = []\n","\n","for row in rows:\n","    poi_id = row.POI_ID\n","    text = row.full_text if row.full_text else ''\n","    if text.strip() == '':\n","        embedding = np.zeros(768)\n","    else:\n","        embedding = get_bert_embedding(text)[0]\n","    poi_embeddings[poi_id] = embedding\n","\n","    # Coordenadas\n","    coord = poi_coords.get(poi_id, (0.0, 0.0))\n","    poi_coord_list.append(coord)\n","    poi_id_list.append(poi_id)\n","\n","# Escalar coordenadas\n","coords_matrix = np.array(poi_coord_list)\n","scaler = StandardScaler()\n","coords_scaled = scaler.fit_transform(coords_matrix)\n","\n","# Matriz embeddings\n","embeddings_matrix = np.array([poi_embeddings[poi_id] for poi_id in poi_id_list])\n","\n","# Concatenar embeddings + coords escaladas\n","X_cluster = np.hstack((embeddings_matrix, coords_scaled))\n","\n","# Clustering\n","kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n","labels = kmeans.fit_predict(X_cluster)\n","\n","# Resultado\n","cluster_rows = []\n","for poi_id, cluster in zip(poi_id_list, labels):\n","    cluster_rows.append({\n","        'POI_ID': poi_id,\n","        'nombre': poi_nombres.get(poi_id, \"NULL\"),\n","        'cluster': int(cluster)\n","    })\n","\n","cluster_df = pd.DataFrame(cluster_rows)\n","\n","# Guardar SQL inserts para tabla Recomendaciones\n","output_sql = \"recomendaciones.sql\"\n","with open(output_sql, \"w\", encoding=\"utf-8\") as f:\n","    f.write(\"DELETE FROM Recomendaciones;\\n\\n\")\n","\n","    f.write(\"INSERT INTO Recomendaciones (POI_ID, nombre, cluster) VALUES\\n\")\n","\n","    inserts = []\n","    for _, row in cluster_df.iterrows():\n","        poi_id = row['POI_ID']\n","        cluster = row['cluster']\n","        nombre = row['nombre'].replace(\"'\", \"''\")\n","        inserts.append(f\"({poi_id}, '{nombre}', {cluster})\")\n","\n","    f.write(\",\\n\".join(inserts) + \";\\n\")\n","\n","print(f\"Archivo '{output_sql}' generado correctamente.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JbCQrPBgpP_M","executionInfo":{"status":"ok","timestamp":1751627127317,"user_tz":-120,"elapsed":64875,"user":{"displayName":"Jenifer Vasquez","userId":"07982682319972899452"}},"outputId":"87d443b1-7622-4267-ff85-40894654328d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Archivo 'recomendaciones.sql' generado correctamente.\n"]}]},{"cell_type":"markdown","source":["# Carga del fichero con las recomendaciones (clusters)"],"metadata":{"id":"aSlLPeLKmRxV"}},{"cell_type":"code","source":["cargar_sql_en_bd(\"recomendaciones.sql\", cursor, conn)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TYWFCLs6p7-C","executionInfo":{"status":"ok","timestamp":1751627365096,"user_tz":-120,"elapsed":474,"user":{"displayName":"Jenifer Vasquez","userId":"07982682319972899452"}},"outputId":"b5541537-7c9e-44e7-e852-310abbe4f5d4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Archivo 'recomendaciones.sql' cargado correctamente en la base de datos.\n"]}]}]}