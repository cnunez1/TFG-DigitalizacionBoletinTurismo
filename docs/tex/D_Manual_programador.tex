\apendice{Documentación técnica de programación}

\section{Introducción}

En este anexo se presentan los aspectos relacionados con la estructura, instalación y ejecución del proyecto desde un punto de vista técnico.
Se incluyen detalles relacionados a la estructura del repositorio que contiene el código fuente del proyecto o los pasos a seguir para ejecutar el mismo.

\section{Estructura de directorios}

Para alojar los ficheros fuente de este proyecto se ha creado un repositorio en GitHub\cite{repositorio}. 
Se ha tratado de seguir una estructura de directorios que permita una fácil comprensión de cada parte del proyecto.
A continuación se explica la estructura de directorios del proyecto y su contenido:

\begin{itemize}
    \item \textbf{.scannerwork/}: Contiene los ficheros de configuración del escáner de SonarQube utilizado para el análisis de código de forma local.
    \item \textbf{data/}: Contiene ficheros de datos con información sobre puntos de interés en Burgos.
        \begin{itemize}
            \item \textbf{burgos\_pois.json}: Fichero JSON con los nodos extraídos de la OpenStreetMaps a partir de Overpass. 
            \item \textbf{pois\_details.csv}: Fichero CSV con las coordenadas y placeID de los puntos de interés obtenidos a partir del JSON anterior.
        \end{itemize}
    \item \textbf{docs/}: Contiene todo lo utilizado para generar la memoria y los anexos.
        \begin{itemize}
            \item \textbf{img/}: Contiene las imágenes utilizadas en la memoria y anexos.
            \item \textbf{tex/}: Contiene los ficheros \LaTeX que componen la memoria y los anexos.
        \end{itemize}
        Además, incluye los ficheros de bibliografía en formato .bib y la memoria y anexos en formato PDF.
    \item \textbf{powerbi/}: Contiene los ficheros relacionados al cuadro de mando de PowerBI.
        \begin{itemize}
            \item \textbf{dashboards/}: Contiene el fichero .pbix del cuadro de mando.
            \item \textbf{icons/}: Contiene los iconos utilizados en el cuadro de mando, extraídos de icons8.\cite{icons8}
            \item \textbf{themes/}: Contiene el tema\cite{theme} utilizado en el cuadro de mando.
        \end{itemize}
    \item \textbf{model/}: Contiene los ficheros relacionados con el modelo obtenido tras el entrenamiento de la red neuronal.
        \begin{itemize}
            \item \textbf{bert\_tokenizer.pkl}: Fichero con el tokenizer obtenido tras el entrenamiento.
            \item \textbf{label\_encoder.pkl}: Fichero con el encoder de las etiquetas de clasificación.
            \item \textbf{tfidf\_vectorizer.pkl}: Fichero con los vectorizers obtenidos tras el entrenamiento.
            \item \textbf{review\_model\_combined\_espanol.keras}: Fichero con el modelo entrenado.
        \end{itemize}
    \item \textbf{sql/}: Contiene los ficheros SQL utilizados para inicializar la base de datos azure.
        \begin{itemize}
            \item \textbf{poi\_review\_inserts.sql}: Fichero SQL con las tablas POI, reviews y algunos inserts.
            \item \textbf{categories.sql}: Fichero SQL con los distintos niveles y sus categorías.
        \end{itemize}
    \item \textbf{src/}: Contiene el código fuente del proyecto.
        \begin{itemize}
            \item \textbf{dataExtraction/}: Contiene los scripts de Python utlilizados para la extracción de puntos de interés.
                \begin{itemize}
                    \item \textbf{OverpassExtraction.py}: Script de Python que utiliza la API de Overpass Turbo para extraer los nodos de OpenStreetMaps de puntos de interés de Burgos.
                    \item \textbf{PlaceIdExtraction.py}: Script de Python que utiliza la API de Google Places para obtener el placeID de los puntos de interés extraídos del script anterior.
                    \item \textbf{ApifyReviewExtraction.py}: Script de Python que utiliza la API de Apify para extraer las reseñas de los puntos de interés.
                \end{itemize}
            \item \textbf{preprocessing/}: Contiene los scripts de Python utilizados para el preprocesamiento de los datos.
                \begin{itemize}
                    \item \textbf{processReviews.py}: Script de Python que procesa las reseñas obtenidas de los ficheros CSV de Google Maps Review Scraper. No es necesario si se usa el script anterior de Apify.
                    \item \textbf{translateReviews.py}: Script de Python que traduce las reseñas que no estén en español y crea un campo 'text\_original' con el texto original.
                    \item \textbf{cleanReviews.py}: Script de Python que limpia las reseñas eliminando saltos de línea y reseñas cortas.
                    \item \textbf{main.py}: Script de Python que ejecuta todo el pipeline de scripts anterior (solo los de preprocesamiento).
                    \item \textbf{mainApify.py}: Script de Python que ejecuta todo el pipeline de scripts anterior (solo los de preprocesamiento) sin el primer paso ya que no es necesario si se usa el script de Apify.
                \end{itemize}
            \item \textbf{finalInserts.ipynb}: Notebook de Jupyter para entrenar la red neuronal, realizar la clasificación de los puntos de interés y el sistema de recomendación.
            \item \textbf{neuralNetwork.py}: Script de Python que contiene la red neuronal ya integrada en el notebook anterior.
            \item \textbf{trainingDataset.csv}: Fichero CSV con el dataset creado y utilizado para el entrenamiento de la red neuronal.
        \end{itemize}
    \item \textbf{README.md}: Fichero de texto markdown que contiene una descripción de como ejecutar el proyecto y sus requisitos.
    \item \textbf{LICENSE}: Fichero de texto que contiene la licencia del proyecto.
    \item \textbf{requirements.txt}: Fichero de texto que contiene las dependencias del proyecto.
    \item \textbf{docker-compose.yml}: Fichero de configuracion de Docker utilizado para levantar SonarQube.
    \item \textbf{sonar-project.properties}: Fichero de configuración de SonarQube para poder usar el escáner.
\end{itemize}

\section{Manual del programador}

En este apartado se explica todo lo necesario para poder ejecutar el proyecto y sus scripts.
Es necesario seguir estas indicaciones antes de continuar con el proceso de instalación y ejecución del proyecto.

\subsection{Entorno de desarrollo}

Este proyecto ha sido desarrollado con diferentes programas y tecnologías detallados a continuación.

\begin{itemize}
    \item \textbf{Python 3.10}: Lenguaje de programación utilizado para el desarrollo y ejecución de los scripts de extracción y preprocesamiento de datos.
    \item \textbf{Google Colab}: Entorno de desarrollo ofrecido por Google utilizado para ejecutar el fichero .ipynb sobre una GPU con el objetivo de acelerar su ejecución.
    \item \textbf{SQL Server Management Studio:} Herramienta ofrecida por Microsoft para la gestión de la base de datos SQL. Permite realizar consultas tras introducir las credenciales correspondientes.
    \item \textbf{Power BI Desktop}: Herramienta de Microsoft utilizada para crear el cuadro de mando. Permite integración con la base de datos.
    \item \textbf{Git}: Sistema de control de versiones utilizado para hacer cambios en el repositorio de GitHub.
    \item \textbf{Visual Studio Code}: Editor de código utilizado para el desarrollo de los ficheros fuente.
    \item \textbf{LaTeX y MikTeX}: Herramientas utilizadas para la generación de la memoria y anexos del proyecto.
\end{itemize}

Además, se han utlizado dos APIs para la extracción de datos a partir de una API Key que se puede obtener en sus páginas web:
\begin{itemize}
    \item \textbf{Google Places API}: API utilizada para extraer los placeId de los puntos de interés de Burgos. No es necesario reutilizarla inmediatamente ya que ya han sido extraídos los puntos de interés. 
    \item \textbf{Apify API}: API utilizada para extraer las reseñas de los puntos de interés. Es necesaria una Key para utilizar el fichero \texttt{ApifyReviewExtraction.py} y se puede obtener en su página web.
\end{itemize}

\section{Compilación, instalación y ejecución del proyecto}

En este apartado se describe el proceso completo de instalación y ejecución del proyecto.
Cabe destacar que no es necesaria la completa ejecución de todos los scripts para poder ejecutar el proyecto.

\subsection{Instalación}

El primer requisito fundamental para la correcta ejecución del proyecto es tener instalado Python 3.10 o superior. 
Además, es necesario instalar las dependencias encontradas en el fichero \texttt{requirements.txt} que se encuentra en la raíz del proyecto.
Estas dependencias se pueden instalar con el siguiente comando:
    
\begin{verbatim}
    pip install -r requirements.txt
\end{verbatim}

Ya vienen definidas las versiones de las dependencias necesarias. El fichero está dividido en dos partes, una para las dependencias de los scripts de Python y otra para las del notebook de Jupyter que he ejecutado en Google Colab.
Estas últimas no son necesarias si se ejecuta el código allí. Por el contrario, si se ejecuta el notebook en local, es necesario instalarlas también. Para ello solo hay que eliminar el '\#' del principio de las líneas del segundo bloque del fichero y ejecutar el comando anterior.

\subsection{Ejecución}

Todos los scripts explicados a continuación se pueden ejecutar desde la terminal siempre que nos encontremos en su directorio con el comando:

\begin{verbatim}
    python3 <nombre_del_script.py>
\end{verbatim}

\subsubsection{Extracción de datos}

\subsection{Pipeline de extracción de datos}

\subsubsection{Vía Apify}

Para la extracción de datos se han desarrollado varios scripts en Python que de ejecutarse ordenadamente permiten obtener las reseñas de los POIs de los municipios de la provincia de Burgos teniendo en cuenta la limitación de reseñas según el plan de pago de Apify.

El primer script a ejecutar es \texttt{OverpassExtraction.py}, que se encarga de extraer los nodos, vías y relaciones de los municipios de la provincia de Burgos. 
Este script utiliza la API de Overpass para obtener los límites de los municipios y guardarlos en un fichero JSON.
Este script funciona a través de una consulta en Overpass QL. Ya que no todos los nodos de que ofrece OpenStreetMaps (OSM) son puntos de interés en Google Maps y por lo tanto no tienen reseñas, se eliminan en la consulta para reducir el tamaño del fichero JSON.
Algunos de los nodos que se eliminan son papeleras, bancos, plazas de parking individuales, etc.
El parámetro \texttt{admin\_level} se utiliza para determinar el nivel de administración del área de la consulta. Se establece en 6 ya que es el nivel provincial lo que permite buscar dentro de los límites de la provincia de Burgos.

El segundo script a ejecutar es \texttt{PlaceIdExtraction.py} que se encarga de extraer los placeIds de los puntos de interés (POIs) obtenidos en el fichero JSON generado por el primer script.
Este script utiliza la API de Google Places para obtener los placeIds de los POIs y guardarlos en un fichero CSV.
Para ello se utiliza la función nearbySearch de la API de Google Places que permite buscar lugares cercanos a unas coordenadas dadas. Estas coordenadas se encuentran en el fichero JSON generado anteriormente por lo que basta 
con recorrer el fichero y realizar una búsqueda para cada uno de los POIs.

El tercer script a ejecutar es \texttt{ApifyReviewExtraction.py} que se encarga de extraer las reseñas de los POIs obtenidos en el fichero CSV generado por el segundo script.
Este script utiliza la API de Apify para extraer las reseñas de los POIs y guardarlas en un fichero CSV.
Para ello se utiliza el Google Maps Reviews Scraper de Apify que permite extraer las reseñas utilizando los placeIds obtenidos anteriormente.
Este scraper devuelve una gran cantidad de campos de información, en mi caso he decidido filtrar solo los más relevantes para el proyecto como pueden ser la valoración o el nombre del punto de interés.
También permite determinar el número de reseñas a extraer por cada placeId. En caso de querer extraer todas hay que escribir 99999.

Las Keys necesarias para su ejecución se deben introducir en el fichero \texttt{.env} que se encuentra en la raíz del proyecto.

\subsubsection{Vía Google Maps Review Scraper}

Este scraper, disponible en GitHub, permite extraer reseñas a partir de una consulta mediante scraping.
Devuelve varios ficheros CSV con multitud de campos. Su instalación viene detallada en el propio repositorio.\cite{scraper}

\subsection{Pipeline de preprocesamiento de datos}

Este pipeline está formado por tres scripts de Python:

El primer script es \texttt{processReviews.py} que se encarga de procesar los ficheros CSV obtenidos del Google Maps Review Scraper para obtener un único fichero CSV con las reseñas y los POIs.
Este script no es necesario si se ha utilizado el scraper de Apify ya que este último ya genera un fichero CSV con las reseñas y los POIs.

El segundo script es \texttt{translateReviews.py} que se encarga de detectar el idioma de las reseñas y traducirlas al español si es necesario.
Para ello se utiliza la API de Google Translate que permite detectar el idioma de un texto y traducirlo a otro idioma.
Este script también crea un campo \texttt{text\_original} que contiene el texto original de la reseña sin traducir.
Esto es útil para no perder la reseña original y poder compararla con la traducción.

El tercer script es \texttt{cleanReviews.py} que se encarga de limpiar las reseñas eliminando saltos de línea y reseñas con menos de 15 palabras.
Esto es útil para garantizar que las reseñas contengan información suficiente para el análisis posterior.

Dependiendo de si se ha utilizado el scraper de Apify o el Google Maps Review Scraper, hay dos scripts principales que ejecutan el pipeline correspondiente:

\begin{itemize}
    \item \texttt{main.py}: Este script ejecuta el pipeline de preprocesamiento de datos utilizando los ficheros CSV obtenidos del Google Maps Review Scraper.
    \item \texttt{mainApify.py}: Este script ejecuta el pipeline de preprocesamiento de datos utilizando los ficheros CSV obtenidos del scraper de Apify.
\end{itemize}

La única diferencia entre ambos es que en el script de Apify se omite el primer script de preprocesado.

El script \texttt{main.py} cuenta con varios parámetros a tener en cuenta que se definen en el propio código:
\begin{itemize}
    \item \texttt{csv\_reviews}: Fichero CSV de entrada con las reseñas obtenidas del Google Maps Review Scraper. Este fichero debe estar dentro de la carpeta \texttt{inputs/} que se encuentra al mismo nivel que el propio script.
    \item \texttt{csv\_poi}: Fichero CSV de entrada con los POIs obtenidos del Google Maps Review Scraper. Este fichero debe estar dentro de la carpeta \texttt{inputs/} que se encuentra al mismo nivel que el propio script.
    \item \texttt{estado}: Provincia a escribir en el fichero de salida. Como todas las reseñas son de la provincia de Burgos se deja con ese valor.
    \item \texttt{max\_reviews}: Número máximo de reseñas a extraer al finalizar el pipeline. Normalmente es irrelevante pero se ha utilizado para crear un dataset equilibrado.
\end{itemize}

\subsection{Aprendizaje automático}

Para ejecutar el fichero \texttt{finalInserts.ipynb} es necesario tener una cuenta de Google y acceder a Google Colab con ella.
En este fichero se entrena la red neuronal y se obtienen las predicciones y las recomendaciones.
Para entrenar la red neuronal, es necesario cargar el fichero \texttt{trainingDataset.csv} que se encuentra en la carpeta \texttt{src/} del proyecto.
Además, hay que cargar el fichero \texttt{.env} que contiene las credenciales de la base de datos.
Es necesario ejecutar las dos primeras celdas del notebook para descargar el connector de la base de datos y las dependencias necesarias.
También es importante seleccionar el entorno de ejecución con GPU para acelerar el entrenamiento de la red neuronal o sus predicciones.

Si se desea saltar el entrenamiento de la red neuronal, se pueden cargar directamente los ficheros del modelo que se encuentran en la carpeta \texttt{model/} del proyecto.
Se puede cargar un fichero .csv con las reseñas y los POIs para realizar las predicciones y recomendaciones.
El formato recomendado es el obtenido tras ejecutar el pipeline de preprocesamiento de datos. Este pipeline devuelve como resultado un fichero \texttt{clean\_reviews.csv} que es el que hay que cargar en Google Colab.
Al ejecutar la celda de las predicciones, se generará un fichero \texttt{datos.sql} que se puede cargar en la base de datos para insertar las predicciones.

Respecto a las recomendaciones, no es necesario cargar ningún fichero ya que se generan en base al contenido de la base de datos. 
Al ejecutar su celda, se generará un fichero \texttt{recomendaciones.sql} que se puede cargar en la base de datos para insertar las recomendaciones.
Además, se muestra en una gráfica el método del codo. Este método se usa para calcular el número óptimo de clusters para las recomendaciones.
Se toma como valor el punto en el que se empieza a estabilizar la curva. Por esa razón, se han utilizado 7 clusters.

\imagen{codo}{Método del codo}{1}

En el notebook se incluye una función \texttt{cargar\_sql\_en\_bd} y las llamadas correspondientes para cargar los ficheros de datos y recomendaciones.
Esta función necesita tres parámetros:
\begin{itemize}
    \item \texttt{ruta\_sql}: Es la ruta del fichero SQL a cargar.
    \item \texttt{cursor}: Es el cursor de la conexión a la base de datos. Se crea durante la ejecución de las predicciones y/o recomendaciones.
    \item \texttt{conn}: Es la conexión a la base de datos. Se crea durante la ejecución de las predicciones y/o recomendaciones.
\end{itemize}   

\subsection{Cuadro de mando}

Si se quiere ejecutar el cuadro de mando de Power BI, es necesario tener instalado Power BI Desktop.
En caso de haberlo hecho, simplemente hay que abrir el fichero \texttt{dashboardFinal.pbix} que se encuentra en la carpeta \texttt{powerbi/dashboards/} del proyecto.
Ahí se pueden ver los resultados finales. Para actualizar los datos tras cargarlos previamente en la base de datos desde Google Colab, solo hay que pulsar el botón de actualizar en la pestaña de Inicio en el menú superior.
Dado que el cuadro de mando está conectado a la base de datos, se actualizarán los datos automáticamente y se podrán ver las predicciones y recomendaciones en el cuadro de mando solo en caso de que se hayan introducido las credenciales de Azure previamente.

\imagen{dashboard}{Cuadro de mando final}{1}

El cuadro de mando se encuentra publicado actualmente en PowerPages de forma privada debido a restricciones de permisos relacionados con las cuentas de la Universidad.
Para publicarlo, hay que ir a archivo y seleccionar Publicar en la web. 
A continuación, se debe ir al portal de Power BI en el navegador y seleccionar el cuadro de mando publicado y en archivo, insertar informe, sitio web o portal.
Esto generará una URL y un código HTML que se puede incrustar en una página web ya sea de PowerPages o de cualquier otro sitio web. 

\section{Pruebas del sistema}

En relación a las pruebas del sistema, se han realizado varias pruebas manuales con el fin de 
verificar el correcto funcionamiento de los scripts y del cuadro de mando.

La prueba más importante ha sido la relacionada con la red neuronal.
Para seleccionar los hiperparámetros, se ha realizado un random search con el fin de obtener los mejores hiperparámetros posibles.

Además, se ha probado con varios conjuntos de datos para comprobar como evolucionaban las métricas de la red neuronal.
Esto ha llevado a la conclusión de intentar construir un dataset multiclase equilibrado con el fin de que la red neuronal pueda aprender de todas las clases por igual.
Para ello, se ha utilizado el parámetro \texttt{max\_reviews} del script \texttt{main.py}.