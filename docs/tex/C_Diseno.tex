\apendice{Especificación de diseño}

\section{Introducción}

En este apéndice se detallan los aspectos de diseño del sistema, incluyendo el diseño de datos, arquitectónico y procedimental. 
Esto se acompañará de diagramas que faciliten la comprensión de la estructura y funcionamiento del sistema.
También se incluyen capturas de pantalla de la aplicación final para poder visualizar su interfaz y funcionalidades.

\section{Diseño de datos}

En este proyecto, se ha optado por utilizar varias opciones para tratar con los datos dependiendo de la fase del proyecto en la que se trabaje.

\subsection{Fase de extracción de datos}

\subsubsection{Vía Apify}

Para comenzar esta fase se ha realizado una consulta la API de Overpass para obtener nodos de OSM. Estos datos se recogen en un fichero JSON que posteriormente se procesa para extraer la información relevante.
Este fichero JSON está formado por campos y valores. Hay una clave general llamada 'elements' que contiene una lista de nodos como elementos individuales. Cada nodo tiene varios campos, como 'id', 'lat', 'lon', 'tags', etc. Los tags son un diccionario que contiene pares clave-valor, donde las claves son los nombres de los atributos y los valores son sus correspondientes valores. 

Una vez obtenido el JSON, se ha pasado por la API de Google para obtener placeIds a partir de las coordenadas obteniendo un fichero CSV con coordenadas y placeIds.
Finalmente, se procesa este fichero CSV por un script de Python que realiza llamadas a la API de Apify para recoger las reseñas y recursos en otro fichero CSV. Este fichero está formado por los siguientes campos: 'categoryName', 'city', 'reviewsCount', 'totalScore', 'stars', 'state', 'text', 'title', 'location', 'originalLanguage', 'publishedAtDate', 'placeId', 'url', 'name'.

\subsubsection{Vía Google Maps Review Scraper}

Al utilizar este scraper, se ha optado por un enfoque diferente. Este scraper devuelve dos ficheros CSV con multitud de campos entre los que se encuentran los recogidos a través de Apify.

\subsection{Fase de preprocesamiento de datos}

El preprocesamiento de datos se divide en tres scripts de Python que se encuentran en el directorio src/dataPreprocessing.

El primer script toma los ficheros CSV obtenidos vía Google Maps Review Scraper y los procesa para obtener un solo fichero CSV similar al obtenido con Apify.
Esto implica que este paso no es necesario si se ha optado por el scraper de Apify. Aquí se añade el campo 'originalLanguage', 'city' y se cambia el formato de la fecha de la review.

El segundo script toma el fichero CSV obtenido vía Apify o vía paso anterior y se encarga de detectar el idioma de las reseñas y de traducirlas a español si es que no están en español. 
Para no perder la reseña original, se añade el campo 'text\_original' que contiene la reseña sin traducir.

Finalmente, el tercer script limpia saltos de línea y valores nulos de las reseñas y elimina las que tienen menos de 15 palabras devolviendo un fichero CSV limpio para
usar el modelo entrenado.

\subsection{Fase de predicciones y recomendaciones}

Esta fase se encuentra tras el entrenamiento del modelo. En ella se obtienen dos ficheros .sql que contienen los 
INSERTS que se utilizarán para cargar en la base de datos los recursos con su categoría y los clusters para las recomendaciones.

\subsection{Modelo de datos}

El modelo de datos utilizado para almacenar toda la información utilizada en el cuadro de mando viene dado por el siguiente esquema.

\imagen{erFinal}{Diagrama entidad-relación}{0.87}

\section{Diseño arquitectónico}

Este proyecto sigue una arquitectura basada en scripts de Python que se ejecutan secuencialmente como un pipeline.
En este enfoque se pueden diferenciar 6 fases del proyecto definidas y diferenciadas claramente por los diferentes scripts:

\subsection{Extracción de datos}

Es la fase inicial del proyecto donde se obtienen los datos de las reseñas y los recursos. 
Se utilizan dos enfoques diferentes: uno a través de la API de Apify y otro a través del Google Maps Review Scraper.
Ambos enfoques generan ficheros CSV con la información necesaria para el análisis posterior.

\subsection{Preprocesamiento de datos}

Es la segunda fase del proyecto donde se procesan los datos obtenidos en la fase anterior.
Se realizan varias tareas como la detección del idioma de las reseñas, la traducción al español, la limpieza de saltos de línea y la eliminación de reseñas con menos de 15 palabras.
Esta fase es crucial para garantizar que los datos estén en un formato adecuado para el análisis posterior.

\subsubsection{Creación del conjunto de datos de entrenamiento}

Asociado a la fase de preprocesamiento se encuentra la creación del dataset de entrenamiento.
Este dataset ha sido creado por mí de forma manual a partir de las reseñas obtenidas en los pasos anteriores.
Para ello, he extraído reseñas de varios recursos seleccionados manualmente a partir de Google Maps y he reetiquetado cada reseña con la categoría del recurso al que pertenece.
Este dataset se encuentra en el directorio \texttt{src/trainingDataset.csv}.

A continuación, se muestran varias métricas obtenidas al analizar este dataset.

\imagen{EDA}{Distribución de reseñas por POI y categoría}{1}
\imagen{EDA2}{Distribución de longitud de texto}{1}

\subsection{Entrenamiento del modelo}

En esta fase se crea la red neuronal y se entrena el modelo con los datos preprocesados. 
Se utiliza el modelo preentrenado de BERT en español y se entrena con los datos de las reseñas.

\subsection{Generación de predicciones}

Una vez entrenado el modelo, se utiliza para generar predicciones sobre las reseñas.
Para ello, se realiza nuevamente una extracción y preprocesado de los recursos y reseñas que se quiera analizar y se pasan por el modelo obtenido para predecir el tipo de recurso.
Finalmente, se obtiene un fichero .sql que se carga a la base de datos en la nube.

\subsection{Sistema de recomendación}

Tras la generación de predicciones, se utiliza nuevamente BERT para generar embeddings de los textos de las reseñas y junto a las coordenadas de los recursos se 
crean varios clusters que agrupan recursos similares.
Nuevamente se obtiene un fichero .sql que se carga a la base de datos en la nube.

\subsection{Análisis y visualización de resultados}

Los resultados obtenidos tanto en la fase de predicción como de recomendación se muestran en el cuadro de mando de Power BI.
Se lee la base de datos en la nube y se muestran lás métricas correspondientes.

\section{Diseño procedimental}

En esta sección se describen los procedimientos de cada script para realizar las diferentes tareas del proyecto.
A continuación se muestran diferentes diagramas de secuencia que ilustran el flujo de trabajo de cada fase del proyecto.

\imagen{sequenceExtraction}{Diagrama de secuencia de la extracción de datos}{1}
\imagen{sequencePreprocessing}{Diagrama de secuencia del preprocesado de datos}{1}
\imagen{sequenceNN}{Diagrama de secuencia de la red neuronal de datos}{1}
\figuraApaisadaSinMarco{1}{sequenceFull}{Diagrama de secuencia completo}{fig:seqFull}{width=\textwidth}