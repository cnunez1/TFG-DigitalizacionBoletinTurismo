\capitulo{5}{Aspectos relevantes del desarrollo del proyecto}

Se presentan a continuación los aspectos más relevantes del desarrollo del proyecto de forma ordenada según las diferentes fases del proyecto.

\section{Extracción de datos}

\subsection{Propuestas de extracción de datos}

A continuación se presentan las diferentes propuestas de extracción de datos que se han considerado para la obtención de los datos necesarios para el desarrollo del proyecto:

\begin{itemize}
    \item \textbf{TripAdvisor API:} La API de TripAdvisor contiene millones de reviews de usuarios sobre destinos turísticos de todo el mundo. Debido a sus límites de reviews\cite{tripadvisor:overview} (5 por lugar, es decir, 5 reviews por solicitud) no es factible.
    Ofrecen 5000 peticiones gratuitas al mes. \cite{tripadvisor:prices}
    \begin{table}[h!]
        \centering
        \begin{tabular}{|c|c|}
            \hline
            \textbf{Solicitudes} & \textbf{Costo por solicitud} \\
            \hline
            0 - 5,000 & €0.00 \\
            5,001 - 20,000 & €0.00876 \\
            20,001 - 100,000 & €0.00815 \\
            100,001 - 500,000 & €0.00762 \\
            500,000+ & €0.00718 \\
            \hline
        \end{tabular}
        \caption{Costos por solicitud en la API de TripAdvisor}
    \end{table}
    \item \textbf{API de Google Places (textSearch) y Google Maps Reviews Scraper de Apify:} Utilizando textSearch se puede obtener un identificador para cada POI de forma gratuita e ilimitada tras crear una cuenta de prueba e introducir una tarjeta de crédito. Para ello, es necesario tener un punto de referencia (coordenadas) y un radio de búsqueda.  
    Los puntos de referencia se pueden obtener mediante una consulta en Overpass Query Language desde su interfaz web para extraer las coordenadas de OSM. 
    
    Al contrario que en el caso anterior, ahora no se emplean los puntos límites de los municipios, solo un punto céntrico de cada uno de ellos.
    El problema en este caso es que no se obtienen todos los POI de un municipio, sino solo los que se encuentran dentro del radio de búsqueda.
    \item \textbf{API de Google Places (nearbySearch) y ficheros binarios de OpenStreetMaps(OSM):} OSM proporciona ficheros binarios (.osm.pbf) que se pueden convertir a GeoJSON para obtener información relevante sobre los municipios como las coordenadas de sus límites. 
    Con la API de Google Places se pueden extraer reviews utilizando nearbySearch que permite obtener las reviews de los POI más relevantes que se encuentren cerca de un punto dado por sus coordenadas. 
    
    Sin embargo, esta API solo permite obtener 5 reviews por lugar y 60 lugares por cada búsqueda alrededor de un punto. Esto implica que si se quiere obtener información de un municipio con más de 60 POI, se deben realizar múltiples búsquedas y muchas peticiones.
    \item \textbf{Consultas de Overpass QL, API de Google Places (nearbySearch) y Google Maps Reviews Scraper de Apify:} A través de la interfaz web de Overpass QL
    al igual que en el caso anterior, se pueden obtener los límites de los municipios mediante consultas en Overpass QL.
    Esta herramienta permite extraer reviews de Google Maps de forma ilimitada y gratuita.

    Con estos identificadores se puede automatizar la extracción de reviews a través de Apify que ofrece 14285 reviews gratuitas por cada cuenta gratuita creada.
    A partir de esas reviews, se debe pagar una cuota mensual teniendo en cuenta que cada 1000 reviews equivalen a 0,35\$.
    Con la cuenta Business esto cambia ya que el precio de una review es de 0,00035\$ mientras que con las anteriores es de 0,0006.\$
    También ofrecen un descuento del 50\% para cuentas educativas\cite{apify:universities}. También ofrecen un descuento de 10\% si se paga anualmente.

    Se muestra una tabla con los precios de las cuentas de Apify\cite{apify:pricing} en dólares américanos:
    \begin{table}[h!]
        \centering
        \begin{tabular}{|c|c|c|c|c|}
            \hline
            \textbf{Tipo de cuenta} & \textbf{Número de reviews} & \textbf{Precio/mes al 50\%} \\
            \hline
            Free & 14285 & 0\$ \\
            Starter & 111428 & 19,5\$ \\
            Scale & 568571 & 99,5\$ \\
            Business & 2854285 & 499,5\$ \\
            Enterprise & Ilimitado & Hablar con Apify \\
            \hline
        \end{tabular}
        \caption{Costos por solicitud en Apify}
    \end{table}
\end{itemize}

\subsubsection{Carga de datos a la base de datos}

Debido a las limitaciones de las propuestas anteriores, únicamente para cargar la base de datos con los recursos a mostrar en el cuadro de mando se ha optado por utilizar un scraper de código abierto disponible en GitHub\cite{scraper} que permite extraer las reseñas y los recursos de forma gratuita y más masiva.
Dado que el proyecto tiene un enfoque educativo, el desarrollador de dicho scraper me ha ofrecido la versión de pago de forma gratuita.

\subsection{Pipeline de extracción de datos}

Para la extracción de datos se han desarrollado varios scripts en Python que de ejecutarse ordenadamente permiten obtener las reseñas de los POIs de los municipios de la provincia de Burgos teniendo en cuenta la limitación de reseñas según el plan de pago de Apify.

El primer script a ejecutar es OverpassExtraction.py, que se encarga de extraer los nodos, vías y relaciones de los municipios de la provincia de Burgos. 
Este script utiliza la API de Overpass para obtener los límites de los municipios y guardarlos en un fichero JSON.
Este script funciona a través de una consulta en Overpass QL. Ya que no todos los nodos de que ofrece OpenStreetMaps (OSM) son puntos de interés en Google Maps y por lo tanto no tienen reseñas, se eliminan en la consulta para reducir el tamaño del fichero JSON.
Algunos de los nodos que se eliminan son papeleras, bancos, plazas de parking individuales, etc.
El parámetro admin\_level se utiliza para determinar el nivel de administración del área de la consulta. Se establece en 6 ya que es el nivel provincial lo que permite buscar dentro de los límites de la provincia de Burgos.

El segundo script a ejecutar es PlaceIdExtraction.py que se encarga de extraer los placeIds de los puntos de interés (POIs) obtenidos en el fichero JSON generado por el primer script.
Este script utiliza la API de Google Places para obtener los placeIds de los POIs y guardarlos en un fichero CSV.
Para ello se utiliza la función nearbySearch de la API de Google Places que permite buscar lugares cercanos a unas coordenadas dadas. Estas coordenadas se encuentran en el fichero JSON generado anteriormente por lo que basta 
con recorrer el fichero y realizar una búsqueda para cada uno de los POIs.

El tercer script a ejecutar es ApifyReviewExtraction.py que se encarga de extraer las reseñas de los POIs obtenidos en el fichero CSV generado por el segundo script.
Este script utiliza la API de Apify para extraer las reseñas de los POIs y guardarlas en un fichero CSV.
Para ello se utiliza el Google Maps Reviews Scraper de Apify que permite extraer las reseñas utilizando los placeIds obtenidos anteriormente.
Este scraper devuelve una gran cantidad de campos de información, en mi caso he decidido filtrar solo los más relevantes para el proyecto como pueden ser la valoración o el nombre del punto de interés.
También permite determinar el número de reseñas a extraer por cada placeId. En caso de querer extraer todas hay que escribir 99999.

\section{Clasificación de los datos}

Con el objetivo de crear un conjunto de datos para entrenar la red neuronal, se han extraído reseñas de puntos de interés con categorías similares 
para después clasificarlas en grupos según su categoría.
Para realizar esta tarea se ha ejecutado el pipeline de extracción de datos descrito anteriormente.
Una vez obtenidas las reseñas se ha montado manualmente un conjunto de datos etiquetado y multiclase con las reseñas obtenidas.

\subsection{Exploratory Data Analysis (EDA)}

\section{Aprendizaje automático}

Para el entrenamiento de la red neuronal se ha utilizado el conjunto de datos etiquetado y multiclase obtenido en la fase anterior.
Debido al tiempo de ejecución del entrenamiento de la red neuronal se ha decidido ejecutar el código en Google Colab, un entorno en línea
ofrecido por Google de forma gratuita que permite ejecutar código Python en la nube y utilizar GPUs para acelerar el entrenamiento de modelos de aprendizaje automático.
El código se ha desarrollado en Python utilizando las librerías TensorFlow y Keras para el entrenamiento de la red neuronal.

Primero se carga el dataset en un DataFrame de Pandas y a continuación se realiza un preprocesamiento con el objetivo de eliminar filas con valores nulos o vacías que no aportan información relevante al modelo.
A continuación, se realiza una codificación a números enteros de las etiquetas de las reseñas con el LabelEncoder de la librería Scikit-learn.
Después se cargan el modelo y tokenizer BERT ya preentrenados para utilizarlos en el preprocesamiento de las reseñas. (dccuchile/bert-base-spanish-wwm-cased)

Se divide el dataset en un conjunto de entrenamiento, validación y test utilizando la función train\_test\_split de Scikit-learn.
A continuación, se realiza un proceso de data augmentation para aumentar el número de reseñas en el conjunto de entrenamiento.
Este proceso está basado en cambiar sinónimos de palabras en las reseñas para generar nuevas reseñas que aporten más información al modelo evitando usar stopwords.

Se crea un vectorizador de texto TF-IDF para transformar las reseñas en vectores numéricos que puedan ser utilizados por el modelo de red neuronal.
Para obtener los embeddings de los subconjuntos de datos, se utiliza el modelo BERT preentrenado y el vectorizador de texto TF-IDF para cada subconjunto, concatenando ambos vectores al final del proceso en cada caso.

Cabe destacar que los siguientes hiperparámetros se han obtenido a través de un proceso de random search con el objetivo de encontrar la mejor combinación de hiperparámetros para el modelo.

Para acabar se crea el modelo de la red neuronal. Las capas del modelo son las siguientes:
\begin{itemize}
    \item Capa densa con 512 unidades y función de activación ReLU.
    \item Capa de dropout con una tasa de 0,3 para evitar el sobreajuste del modelo.
    \item Capa densa con 64 unidades y activación ReLU.
    \item Capa de dropout con una tasa de 0,2 para evitar el sobreajuste del modelo.
    \item Capa de salida en forma de capa densa con una activación softmax para la clasificación multiclase.
\end{itemize}

El modelo se compila con el optimizador Adam con una tasa de aprendizaje de 0.00017954 y la función de pérdida categorical\_crossentropy.
Finalmente, se entrena el modelo con 15 epochs y un tamaño de lote de 32 aplicando los callbacks definidos anteriormente.

Previamente al entrenamiento del modelo se define una función de callback para que el modelo se detenga con Early Stopping si la validation\_loss no disminuye. 
De la misma forma, se define otro callback para disminuir la tasa de aprendizaje en caso de ser necesario.

Finalmente se entrena el modelo con el conjunto de entrenamiento y validación con 25 epochs y un tamaño de lote de 32.
Tras su finalización, se obtiene el informe de clasificación del modelo y la matriz de confusión y se guarda el modelo entrenado para su uso posterior.

\subsubsection{Resultados del entrenamiento}

\section{Análisis de reseñas}

\subsection{Modelo de base de datos}

Con el objetivo de recoger métricas interesantes sobre las reseñas extraídas en la primera fase, se ha creado un cuadro de mando en Power BI.
Este cuadro de mando contiene diferentes visualizaciones que permiten analizar las reseñas extraídas de los puntos de interés de la provincia de Burgos.
Se ha creado un script que se encarga de extraer el nombre de las ciudades o municipios y el nombre de los puntos de interés y guardarlos en la tabla POI.
Además, se ha creado otro script que se encarga de extraer información relevante sobre las reseñas: el texto de la reseña, la categoría ofrecida por Apify, la valoración, el nombre del POI y el nombre del usuario.
Previamente a insertar esta información en la tabla reviews, se usa la biblioteca gender\_guesser para predecir el génerio del usuario y guardarlo junto al resto de información en la tabla.
Finalmente se ha creado la base de datos en MySQL de forma local y se ha importado la información.
Para importar los datos, se ha utilizado la base de datos MySQL de forma local y se ha creado una conexión a la base de datos desde Power BI.

La métrica más interesante analizada en Power BI es el TORI. Su fórmula es la siguiente:
\begin{equation}
    TORI(d_n, c_m) = \sum_{ap \in AP_{d_n, c_m}} \frac{\left( TS(ap) + \frac{5}{4} \cdot FP(ap) \right) \cdot N(ap)}{\sum_{ap \in AP_{d_n, c_m}} N(ap)}
\end{equation} 

Donde $D$ es el conjunto de destinos, $C$ es el conjunto de categorías, $AP_{d_n,c_m}$ es el conjunto de reseñas de un destino $d_n$ y una categoría $c_m$, $TS(ap)$ es la puntuación de la reseña $ap$ y $N(ap)$ es el número de reseñas de la reseña $ap$.
El objetivo de esta métrica es comparar de forma objetiva los puntos de interés o destinos turísticos y determinar cuáles son los más relevantes en función de las reseñas obtenidas.
Estos calculos se realizan de forma automática en Power BI a través	de medidas DAX que permiten realizar cálculos complejos sobre los datos importados.

Además de esto, se han creado visualizaciones para ver la distribución por categorías, fechas y género del usuario de las reseñas obtenidas así como un mapa y una ontología para los tipos de recursos.

\section{Publicación del cuadro de mando}

Finalmente, se ha publicado el cuadro de mando en Power BI para que pueda ser consultado por cualquier persona interesada en el análisis de las reseñas obtenidas.
Se ha utilizado PowerPages para crear una página web que permite acceder al cuadro de mando de forma sencilla y rápida.